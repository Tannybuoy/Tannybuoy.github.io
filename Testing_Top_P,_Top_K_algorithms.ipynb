{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyONk6N0VhYmC+LAx2WwerQX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tannybuoy/Tannybuoy.github.io/blob/master/Testing_Top_P%2C_Top_K_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this FIRST to fix widget metadata\n",
        "import json\n",
        "\n",
        "# Get notebook metadata\n",
        "from google.colab import _message\n",
        "notebook = _message.blocking_request('get_ipynb')\n",
        "\n",
        "# Remove problematic widget metadata\n",
        "if 'metadata' in notebook and 'widgets' in notebook['metadata']:\n",
        "    del notebook['metadata']['widgets']\n",
        "\n",
        "print(\"âœ“ Widget metadata cleared!\")"
      ],
      "metadata": {
        "id": "7gOCks0lZjaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQAJC5tC9gEc"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install dependencies (run once)\n",
        "!pip install transformers torch accelerate -q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n"
      ],
      "metadata": {
        "id": "1ZApYUj49kz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load model\n",
        "print(\"Loading model...\")\n",
        "model_name = \"gpt2\"  # Small, fast model - good for learning\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "print(\"Model loaded!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Bo6XqkGd9oxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this line to fix the warning\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "Klzu5aVP-N-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Generate with top-k\n",
        "def generate_text(prompt, top_k=50, temperature=0.7, max_tokens=50):\n",
        "    \"\"\"Generate text with top-k sampling\"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=top_k,\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "QbLw66te9pyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test it!\n",
        "prompt = \"The future of AI product management is\"\n",
        "result = generate_text(prompt, top_k=50, temperature=0.7)\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "ri8gw8pc9q1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Compare different top-k values\n",
        "print(\"Comparing different top-k values:\\n\")\n",
        "\n",
        "for k in [10, 30, 50, 100]:\n",
        "    print(f\"--- Top-k = {k} ---\")\n",
        "    result = generate_text(prompt, top_k=k, temperature=0.7, max_tokens=30)\n",
        "    print(result)\n",
        "    print()"
      ],
      "metadata": {
        "id": "v_PsXO5w9rxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the same prompt multiple times with the same top-k\n",
        "# to see randomness\n",
        "\n",
        "prompt = \"The future of AI product management is\"\n",
        "\n",
        "print(\"Running top-k=50 five times:\\n\")\n",
        "for i in range(5):\n",
        "    result = generate_text(prompt, top_k=50, temperature=0.7, max_tokens=30)\n",
        "    print(f\"{i+1}. {result}\\n\")"
      ],
      "metadata": {
        "id": "MUZLh7aF-TiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conservative: narrow choices, low randomness\n",
        "generate_text(prompt, top_k=20, temperature=0.5)\n",
        "\n",
        "# Balanced: medium choices, medium randomness\n",
        "generate_text(prompt, top_k=50, temperature=0.7)\n",
        "\n",
        "# Wild: wide choices, high randomness\n",
        "generate_text(prompt, top_k=100, temperature=1.0)"
      ],
      "metadata": {
        "id": "y5x8eZoP-q51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Top-P Generation Function\n",
        "def generate_with_top_p(prompt, top_p=0.9, temperature=0.7, max_tokens=50):\n",
        "    \"\"\"\n",
        "    Generate text with top-p (nucleus) sampling\n",
        "\n",
        "    Args:\n",
        "        prompt: Input text\n",
        "        top_p: Cumulative probability threshold (0.0 to 1.0)\n",
        "               Higher = more diverse, Lower = more focused\n",
        "        temperature: Randomness control\n",
        "        max_tokens: Number of tokens to generate\n",
        "    \"\"\"\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    output = model.generate(\n",
        "        encoded['input_ids'],\n",
        "        attention_mask=encoded['attention_mask'],\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        top_p=top_p,              # ðŸ”‘ This is the key parameter!\n",
        "        temperature=temperature,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "9N8gSIyl-4G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Test it\n",
        "result = generate_with_top_p(\n",
        "    \"The future of AI product management is\",\n",
        "    top_p=0.9,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(result)\n",
        "\n"
      ],
      "metadata": {
        "id": "7JFu4jClAJ4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Compare different top-p values\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARING DIFFERENT TOP-P VALUES\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "prompt = \"The future of AI product management is\"\n",
        "\n",
        "for p in [0.5, 0.7, 0.9, 0.95]:\n",
        "    print(f\"--- Top-p = {p} ---\")\n",
        "    result = generate_with_top_p(prompt, top_p=p, temperature=0.7, max_tokens=40)\n",
        "    print(result)\n",
        "    print()"
      ],
      "metadata": {
        "id": "31aApInSALQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Visual Comparison\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_top_p_vs_top_k(prompt, top_k=50, top_p=0.9):\n",
        "    \"\"\"Visualize how top-k and top-p filter differently\"\"\"\n",
        "\n",
        "    # Get probabilities\n",
        "    encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(encoded['input_ids'])\n",
        "        logits = outputs.logits[0, -1, :]\n",
        "\n",
        "    probs = torch.softmax(logits, dim=0).numpy()\n",
        "    sorted_indices = np.argsort(probs)[::-1]\n",
        "    sorted_probs = probs[sorted_indices]\n",
        "\n",
        "    # Calculate cumulative for top-p\n",
        "    cumsum = np.cumsum(sorted_probs)\n",
        "    top_p_cutoff = np.searchsorted(cumsum, top_p) + 1\n",
        "\n",
        "    # Plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Top-K visualization\n",
        "    x = range(1, 101)\n",
        "    y = sorted_probs[:100]\n",
        "    colors = ['steelblue' if i < top_k else 'lightgray' for i in range(100)]\n",
        "    ax1.bar(x, y, color=colors, alpha=0.7)\n",
        "    ax1.axvline(top_k, color='red', linestyle='--', linewidth=2, label=f'Top-K cutoff ({top_k})')\n",
        "    ax1.set_title(f'Top-K = {top_k} (Fixed Number)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Token Rank')\n",
        "    ax1.set_ylabel('Probability')\n",
        "    ax1.legend()\n",
        "    ax1.text(0.5, 0.95, f'Always uses {top_k} tokens',\n",
        "             transform=ax1.transAxes, ha='center', va='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    # Top-P visualization\n",
        "    colors_p = ['green' if i < top_p_cutoff else 'lightgray' for i in range(100)]\n",
        "    ax2.bar(x, y, color=colors_p, alpha=0.7)\n",
        "    ax2.axvline(top_p_cutoff, color='red', linestyle='--', linewidth=2,\n",
        "                label=f'Top-P cutoff ({top_p_cutoff} tokens for {top_p})')\n",
        "    ax2.set_title(f'Top-P = {top_p} (Adaptive)', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Token Rank')\n",
        "    ax2.set_ylabel('Probability')\n",
        "    ax2.legend()\n",
        "    ax2.text(0.5, 0.95, f'Uses {top_p_cutoff} tokens ({cumsum[top_p_cutoff-1]:.1%} mass)',\n",
        "             transform=ax2.transAxes, ha='center', va='top',\n",
        "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Show actual tokens\n",
        "    print(f\"\\nPrompt: '{prompt}'\\n\")\n",
        "    print(f\"Top-K ({top_k}) would include tokens 1-{top_k}\")\n",
        "    print(f\"Top-P ({top_p}) includes tokens 1-{top_p_cutoff} (reaching {cumsum[top_p_cutoff-1]:.1%} probability)\\n\")\n",
        "\n",
        "    print(\"Top 10 tokens:\")\n",
        "    for i in range(10):\n",
        "        token = tokenizer.decode([sorted_indices[i]])\n",
        "        in_k = \"âœ“\" if i < top_k else \"âœ—\"\n",
        "        in_p = \"âœ“\" if i < top_p_cutoff else \"âœ—\"\n",
        "        print(f\"{i+1}. '{token}' - {sorted_probs[i]:.4f} | Top-K: {in_k} | Top-P: {in_p}\")\n",
        "\n",
        "# Run visualization\n",
        "visualize_top_p_vs_top_k(\"The cat sat on the\", top_k=50, top_p=0.9)"
      ],
      "metadata": {
        "id": "vtI3upPJAO5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Interactive comparison tool\n",
        "def compare_all_methods(prompt, num_samples=3):\n",
        "    \"\"\"Generate with different methods to see the differences\"\"\"\n",
        "\n",
        "    print(f\"Prompt: '{prompt}'\\n\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    methods = [\n",
        "        (\"Greedy (no sampling)\", {\"do_sample\": False}),\n",
        "        (\"Top-K only (k=50)\", {\"do_sample\": True, \"top_k\": 50, \"temperature\": 0.7}),\n",
        "        (\"Top-P only (p=0.9)\", {\"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.7}),\n",
        "        (\"Both (k=50, p=0.9)\", {\"do_sample\": True, \"top_k\": 50, \"top_p\": 0.9, \"temperature\": 0.7}),\n",
        "    ]\n",
        "\n",
        "    for method_name, params in methods:\n",
        "        print(f\"\\nðŸ“Œ {method_name}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "            output = model.generate(\n",
        "                encoded['input_ids'],\n",
        "                attention_mask=encoded['attention_mask'],\n",
        "                max_new_tokens=30,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                **params\n",
        "            )\n",
        "            result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            print(f\"{i+1}. {result}\")\n",
        "        print()\n",
        "\n",
        "# Run comparison\n",
        "compare_all_methods(\"The hardest part of being a PM is\", num_samples=2)"
      ],
      "metadata": {
        "id": "ux6l77pdAk4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install JupyterLite\n",
        "!pip install jupyterlite-core\n",
        "\n"
      ],
      "metadata": {
        "id": "TVU71z-JAxq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Llz8vwzcBaeD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}